!pip install -q torch transformers sentence-transformers pandas openpyxl tqdm scikit-learn 2>/dev/null

import os, re, time, torch
import numpy as np
import pandas as pd
import warnings
from tqdm.auto import tqdm
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from collections import defaultdict
from google.colab import files

warnings.filterwarnings("ignore")

# ==================== CONFIGURACIÃ“N ====================
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ðŸš€ Dispositivo: {device.upper()}")

# MODELO 1: E5 para clustering (mantiene agrupaciÃ³n rÃ¡pida)
print("ðŸ“¦ Cargando multilingual-e5-large para clustering...")
embedder = SentenceTransformer("intfloat/multilingual-e5-large", device=device)

# MODELO 2: RoBERTa-spanish para anÃ¡lisis de sentimiento
print("ðŸ“¦ Cargando RoBERTa-spanish para anÃ¡lisis de tono...")
sentiment_model_name = "finiteautomata/beto-sentiment-analysis"
tokenizer = AutoTokenizer.from_pretrained(sentiment_model_name)
sentiment_model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_name)
sentiment_model = sentiment_model.to(device)
sentiment_model.eval()

print("âœ… Modelos cargados\n")

# ==================== REGLAS COMPLEMENTARIAS ====================
# Reglas que REFUERZAN el anÃ¡lisis de RoBERTa

CLIENT_POSITIVE_PATTERNS = [
    "liderazgo del", "bajo el liderazgo", "gracias a la alcaldÃ­a",
    "gracias al alcalde", "gestiÃ³n del alcalde", "alcaldÃ­a entrega",
    "alcalde confirma", "alcalde anuncia", "alcalde inaugura"
]

CLIENT_NEGATIVE_PATTERNS = [
    "investigan al alcalde", "denuncian al", "escÃ¡ndalo del alcalde",
    "corrupciÃ³n del", "acusan al alcalde", "irregularidades del"
]

NEUTRAL_REACTIONS = [
    "lamenta", "rechaza atentado", "condena", "expresa solidaridad",
    "pide", "solicita", "manifiesta preocupaciÃ³n"
]

ACHIEVEMENT_INDICATORS = [
    "bachilleres inician", "estudiantes se inscriben", "familias beneficiadas",
    "viviendas entregadas", "obras culminadas", "inversiÃ³n de",
    "programas profesionales", "vida universitaria", "acceso a"
]

TOPICS_KEYWORDS = {
    "Infraestructura": ["escenario", "multideportivo", "obra", "infraestructura", "vÃ­a", "parque",
                        "pavimentaciÃ³n", "construcciÃ³n", "puente", "polideportivo", "cancha"],
    "EducaciÃ³n": ["educaciÃ³n", "bachilleres", "universitaria", "colegio", "becas", "estudiantes",
                  "escuela", "universidad", "educaciÃ³n superior", "acceso a la universidad"],
    "Seguridad y Justicia": ["policÃ­a", "seguridad", "auxiliares", "dotaciÃ³n", "homicidio",
                            "delincuencia", "crimen", "atentado", "fallecimiento"],
    "Relaciones Internacionales": ["embajador", "cumbre", "celac", "diplomÃ¡tico", "internacional", "bilateral"],
    "CorrupciÃ³n y EscÃ¡ndalos": ["escÃ¡ndalo", "corrupciÃ³n", "investigan al", "denuncia contra", "fraude"],
    "Salud": ["salud", "hospital", "mÃ©dico", "atenciÃ³n mÃ©dica", "vacunaciÃ³n", "pandemia"],
    "Medio Ambiente": ["lluvias", "inundaciones", "emergencia", "clima", "ambiental", "desastre natural",
                      "huracÃ¡n", "afectados", "coletazo"],
    "EconomÃ­a": ["economÃ­a", "empleo", "empresa", "comercio", "inversiÃ³n econÃ³mica", "presupuesto"],
}

# ==================== FUNCIONES DE CLUSTERING (E5) ====================

def normalize_text(text):
    """Normaliza texto removiendo artÃ­culos y preposiciones"""
    t = text.lower()
    stopwords = ['el', 'la', 'los', 'las', 'un', 'una', 'unos', 'unas',
                 'de', 'del', 'en', 'para', 'por', 'con', 'sin', 'sobre',
                 'que', 'todo', 'estÃ¡', 'son', 'es']

    t = re.sub(r'[^\wÃ¡Ã©Ã­Ã³ÃºÃ±\s]', ' ', t)
    t = re.sub(r'\s+', ' ', t).strip()

    words = t.split()
    words = [w for w in words if w not in stopwords and len(w) > 2]

    return " ".join(words)

def get_signature(text, words=20):
    """Genera firma textual normalizada para clustering"""
    normalized = normalize_text(text)
    return " ".join(normalized.split()[:words])

def universal_cluster(texts):
    """Clustering usando E5 (mantiene agrupaciÃ³n rÃ¡pida y efectiva)"""
    print("ðŸ“Š Agrupando noticias similares con E5...")
    groups = defaultdict(list)

    for i, t in enumerate(texts):
        signature = get_signature(t)
        groups[signature].append(i)

    representatives = []
    cluster_map = [-1] * len(texts)
    cid = 0

    for idxs in groups.values():
        longest_idx = max(idxs, key=lambda i: len(texts[i]))
        representatives.append(texts[longest_idx])

        for i in idxs:
            cluster_map[i] = cid
        cid += 1

    print(f"âœ… {len(texts)} noticias â†’ {len(representatives)} grupos Ãºnicos")
    return representatives, np.array(cluster_map)

# ==================== ANÃLISIS CON ROBERTA ====================

def get_roberta_sentiment(text, max_length=512):
    """Obtiene sentimiento base usando RoBERTa"""
    # Truncar texto si es muy largo
    text_truncated = text[:max_length * 4]  # Aproximado para tokens

    inputs = tokenizer(text_truncated, return_tensors="pt",
                      truncation=True, max_length=max_length,
                      padding=True)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = sentiment_model(**inputs)
        logits = outputs.logits
        probs = torch.softmax(logits, dim=1)

    # BETO sentiment: 0=NEG, 1=NEU, 2=POS
    sentiment_idx = torch.argmax(probs, dim=1).item()
    confidence = probs[0][sentiment_idx].item()

    sentiment_map = {0: "Negativo", 1: "Neutro", 2: "Positivo"}

    return sentiment_map[sentiment_idx], confidence

def analyze_with_roberta(rep_text, client_name):
    """AnÃ¡lisis hÃ­brido: RoBERTa + reglas contextuales"""
    lower = rep_text.lower()
    client_lower = client_name.lower()

    # PASO 1: Obtener sentimiento base de RoBERTa
    base_sentiment, base_confidence = get_roberta_sentiment(rep_text)

    # PASO 2: Ajustes contextuales especÃ­ficos para marca

    # Caso 1: Negativos DIRECTOS contra el cliente (override RoBERTa)
    if any(pattern in lower for pattern in CLIENT_NEGATIVE_PATTERNS):
        return "Negativo", 0.95, base_sentiment

    # Caso 2: Reacciones neutras (override solo si RoBERTa marcÃ³ positivo/negativo)
    if any(reaction in lower for reaction in NEUTRAL_REACTIONS):
        # El cliente solo reacciona, no gestiona
        if base_sentiment != "Neutro":
            return "Neutro", 0.88, base_sentiment

    # Caso 3: POSITIVOS claros de gestiÃ³n (reforzar si RoBERTa es neutro)
    client_is_positive_subject = any(pattern in lower for pattern in CLIENT_POSITIVE_PATTERNS)
    has_achievements = any(indicator in lower for indicator in ACHIEVEMENT_INDICATORS)

    if client_is_positive_subject or has_achievements:
        if base_sentiment == "Neutro":
            # RoBERTa no captÃ³ el positivo, pero el cliente SÃ estÃ¡ gestionando
            return "Positivo", 0.87, base_sentiment
        elif base_sentiment == "Positivo":
            # RoBERTa captÃ³ correctamente, aumentamos confianza
            return "Positivo", min(0.95, base_confidence + 0.10), base_sentiment

    # PASO 3: Usar anÃ¡lisis de RoBERTa como base
    return base_sentiment, round(base_confidence, 3), base_sentiment

def get_topic(text):
    """AnÃ¡lisis de tema por keywords"""
    lower = text.lower()
    tema = "GestiÃ³n y Acciones"
    max_matches = 0

    for topic, keywords in TOPICS_KEYWORDS.items():
        matches = sum(1 for kw in keywords if kw in lower)
        if matches > max_matches:
            max_matches = matches
            tema = topic

    return tema

# ==================== MAIN EXECUTION ====================
print("\n" + "="*80)
print("ðŸ¤– ANALIZADOR HÃBRIDO v31 - RoBERTa + E5")
print("="*80 + "\n")

client_name = input("ðŸ‘¤ Nombre del cliente (ej: Carlos Pinedo): ").strip()
if not client_name:
    client_name = "Carlos Pinedo"
    print(f"   Usando cliente por defecto: {client_name}")

print("\nðŸ“‚ Sube tu archivo Excel...")
uploaded = files.upload()
file_name = list(uploaded.keys())[0]

print(f"ðŸ“– Leyendo {file_name}...")
df = pd.read_excel(file_name)

text_columns = ['resumen', 'texto', 'contenido', 'titulo', 'title', 'noticia', 'descripcion']
text_col = next((c for c in text_columns if c in df.columns), None)

if not text_col:
    print(f"âŒ Error: No se encontrÃ³ columna de texto. Columnas disponibles: {list(df.columns)}")
    raise ValueError("No se encontrÃ³ columna de texto vÃ¡lida")

print(f"âœ… Columna detectada: '{text_col}'")

df = df.dropna(subset=[text_col]).reset_index(drop=True)
texts = df[text_col].astype(str).tolist()
print(f"ðŸ“Š {len(texts)} noticias cargadas\n")

# ==================== PROCESAMIENTO ====================
t0 = time.time()

# PASO 1: Clustering con E5
rep_texts, cluster_map = universal_cluster(texts)

# PASO 2: AnÃ¡lisis con RoBERTa + reglas
print("\nðŸ¤– Analizando tono con RoBERTa-spanish...")
tonos = []
confianzas = []
tonos_roberta_base = []  # Para comparaciÃ³n
temas = []

for rep in tqdm(rep_texts, desc="Procesando"):
    tono, conf, base_sent = analyze_with_roberta(rep, client_name)
    tema = get_topic(rep)

    tonos.append(tono)
    confianzas.append(conf)
    tonos_roberta_base.append(base_sent)
    temas.append(tema)

# PASO 3: Mapear a todas las noticias del grupo
print("\nðŸ”— Aplicando resultados a grupos similares...")
df["grupo_id"] = cluster_map
df["tono_marca"] = [tonos[cluster_map[i]] for i in range(len(df))]
df["confianza"] = [confianzas[cluster_map[i]] for i in range(len(df))]
df["tema"] = [temas[cluster_map[i]] for i in range(len(df))]
df["tono_roberta_base"] = [tonos_roberta_base[cluster_map[i]] for i in range(len(df))]

# ==================== EXPORTACIÃ“N ====================
out = f"Final_RoBERTa_{client_name.replace(' ', '_')}_{time.strftime('%Y%m%d_%H%M')}.xlsx"
print(f"\nðŸ’¾ Guardando resultados en {out}...")

df.to_excel(out, sheet_name="Resultados", index=False, engine="openpyxl")
files.download(out)

# ==================== RESULTADOS ====================
tiempo_total = time.time() - t0
print(f"\n{'='*80}")
print(f"âœ… Â¡ANÃLISIS COMPLETADO CON RoBERTa!")
print(f"â±ï¸  Tiempo: {tiempo_total:.1f}s")
print(f"ðŸ“„ Archivo: {out}")
print(f"{'='*80}\n")

total = len(df)
pos = (df["tono_marca"] == "Positivo").sum()
neg = (df["tono_marca"] == "Negativo").sum()
neu = (df["tono_marca"] == "Neutro").sum()

# ComparaciÃ³n: ajustes aplicados
ajustes = (df["tono_marca"] != df["tono_roberta_base"]).sum()

print("ðŸ“Š RESUMEN:\n")
print(f"   ðŸ“ˆ Total noticias: {total}")
print(f"   ðŸ”¢ Grupos Ãºnicos: {len(rep_texts)}")
print(f"   ðŸ”— Promedio noticias/grupo: {total/len(rep_texts):.1f}")
print(f"   âœ… Positivas: {pos} ({pos/total*100:.1f}%)")
print(f"   âŒ Negativas: {neg} ({neg/total*100:.1f}%)")
print(f"   âšª Neutras: {neu} ({neu/total*100:.1f}%)")
print(f"\n   ðŸ”§ Ajustes contextuales aplicados: {ajustes} ({ajustes/total*100:.1f}%)")

print("\nðŸ“ˆ DISTRIBUCIÃ“N DE TONO FINAL:")
print(df["tono_marca"].value_counts().to_string())

print("\nðŸ¤– TONO BASE ROBERTA (sin ajustes):")
print(df["tono_roberta_base"].value_counts().to_string())

print("\nðŸŽ¯ DISTRIBUCIÃ“N DE TEMAS:")
print(df["tema"].value_counts().to_string())

# Mostrar ejemplos de ajustes
if ajustes > 0:
    print("\nðŸ” EJEMPLOS DE AJUSTES CONTEXTUALES:")
    adjusted = df[df["tono_marca"] != df["tono_roberta_base"]].head(3)
    for idx, row in adjusted.iterrows():
        texto_corto = row[text_col][:90] + "..." if len(row[text_col]) > 90 else row[text_col]
        print(f"\n   ðŸ“° {texto_corto}")
        print(f"      RoBERTa base: {row['tono_roberta_base']} â†’ Final: {row['tono_marca']}")

print("\nâœ¨ AnÃ¡lisis finalizado exitosamente âœ¨")
