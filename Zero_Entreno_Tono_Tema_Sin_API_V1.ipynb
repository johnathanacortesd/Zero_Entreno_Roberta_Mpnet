{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title üìä Interfaz para Concatenar Columnas (Ejecutar esta celda)\n",
        "# --- Importaciones necesarias ---\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from google.colab import files\n",
        "from IPython.display import display, clear_output\n",
        "import io\n",
        "\n",
        "# --- Instalaci√≥n silenciosa de la dependencia para .xlsx ---\n",
        "!pip install -q openpyxl\n",
        "\n",
        "# --- Clase principal de la aplicaci√≥n ---\n",
        "class VisualConcatenator:\n",
        "    def __init__(self):\n",
        "        \"\"\"Inicializa la interfaz y sus componentes.\"\"\"\n",
        "        self.df = None\n",
        "        self.output_df = None\n",
        "\n",
        "        # --- Definici√≥n de los Widgets (Componentes de la UI) ---\n",
        "        self.title = widgets.HTML(\"<h2>üîó Concatenador Visual de Columnas</h2><hr>\")\n",
        "        self.instructions = widgets.HTML(\"<h4>Paso 1: Sube tu archivo Excel para empezar.</h4>\")\n",
        "\n",
        "        self.uploader = widgets.FileUpload(\n",
        "            accept='.xlsx',\n",
        "            description='Subir Archivo',\n",
        "            button_style='primary'\n",
        "        )\n",
        "\n",
        "        # Contenedor para la segunda parte de la UI (que aparece despu√©s de subir)\n",
        "        self.step2_container = widgets.VBox([])\n",
        "\n",
        "        # √Årea de salida para mensajes y resultados\n",
        "        self.output_area = widgets.Output()\n",
        "\n",
        "        # --- Observador de eventos ---\n",
        "        # Llama a la funci√≥n _handle_upload cuando se sube un archivo\n",
        "        self.uploader.observe(self._handle_upload, names='value')\n",
        "\n",
        "    def _handle_upload(self, change):\n",
        "        \"\"\"Se activa al subir un archivo. Lee el archivo y crea la UI de selecci√≥n.\"\"\"\n",
        "        # Limpia cualquier contenido previo en el √°rea de salida\n",
        "        with self.output_area:\n",
        "            clear_output()\n",
        "\n",
        "        uploaded_file_info = self.uploader.value\n",
        "        if not uploaded_file_info:\n",
        "            return # Si se cancela la subida, no hacer nada\n",
        "\n",
        "        # Obtener el nombre y contenido del archivo subido\n",
        "        filename = list(uploaded_file_info.keys())[0]\n",
        "        content = uploaded_file_info[filename]['content']\n",
        "\n",
        "        try:\n",
        "            with self.output_area:\n",
        "                print(f\"‚è≥ Leyendo '{filename}'...\")\n",
        "                # Leer el archivo Excel desde la memoria\n",
        "                self.df = pd.read_excel(io.BytesIO(content))\n",
        "                print(f\"‚úÖ ¬°Archivo cargado! Se encontraron {self.df.shape[0]} filas.\")\n",
        "                print(f\"üìä Columnas disponibles: {', '.join(self.df.columns)}\")\n",
        "\n",
        "            # Si el archivo se ley√≥ bien, crear la siguiente parte de la interfaz\n",
        "            self._create_column_selector_ui()\n",
        "\n",
        "        except Exception as e:\n",
        "            with self.output_area:\n",
        "                print(f\"‚ùå Error al leer el archivo. Aseg√∫rate de que es un .xlsx v√°lido.\")\n",
        "                print(f\"Detalle: {e}\")\n",
        "\n",
        "    def _create_column_selector_ui(self):\n",
        "        \"\"\"Crea los widgets para seleccionar columnas y procesar.\"\"\"\n",
        "\n",
        "        # Nuevas instrucciones para el usuario\n",
        "        step2_instructions = widgets.HTML(\"\"\"\n",
        "        <h4>Paso 2: Selecciona las columnas a unir.</h4>\n",
        "        <p><i>Usa <b>Ctrl+Click</b> (o <b>Cmd+Click</b> en Mac) para elegir varias.</i></p>\n",
        "        \"\"\")\n",
        "\n",
        "        # Widget para seleccionar m√∫ltiples columnas\n",
        "        self.column_selector = widgets.SelectMultiple(\n",
        "            options=self.df.columns.tolist(),\n",
        "            description='Columnas:',\n",
        "            layout=widgets.Layout(height='180px', width='95%'),\n",
        "            style={'description_width': 'initial'}\n",
        "        )\n",
        "\n",
        "        # Bot√≥n para iniciar el procesamiento\n",
        "        self.process_button = widgets.Button(\n",
        "            description='üîó Procesar y Descargar Resultado',\n",
        "            button_style='success',\n",
        "            icon='cogs',\n",
        "            layout=widgets.Layout(width='300px', height='40px', margin='10px 0 0 0')\n",
        "        )\n",
        "\n",
        "        # Conectar el bot√≥n a la funci√≥n de procesamiento\n",
        "        self.process_button.on_click(self._process_and_download)\n",
        "\n",
        "        # Actualizar el contenedor del Paso 2 con los nuevos widgets\n",
        "        self.step2_container.children = [\n",
        "            step2_instructions,\n",
        "            self.column_selector,\n",
        "            self.process_button\n",
        "        ]\n",
        "\n",
        "    def _process_and_download(self, b):\n",
        "        \"\"\"Realiza la concatenaci√≥n, acortado y descarga del archivo.\"\"\"\n",
        "        with self.output_area:\n",
        "            clear_output()\n",
        "\n",
        "            selected_cols = self.column_selector.value\n",
        "            if len(selected_cols) < 2:\n",
        "                print(\"‚ùå Error: Debes seleccionar al menos dos columnas para poder unirlas.\")\n",
        "                return\n",
        "\n",
        "            print(\"üöÄ Procesando los datos...\")\n",
        "            print(f\"   - Uniendo columnas: {', '.join(selected_cols)}\")\n",
        "\n",
        "            try:\n",
        "                # --- L√≥gica principal ---\n",
        "                # 1. Funci√≥n para acortar el texto a 80 palabras\n",
        "                def acortar_texto(texto, limite=80):\n",
        "                    palabras = str(texto).split()\n",
        "                    if len(palabras) > limite:\n",
        "                        return ' '.join(palabras[:limite]) + '...'\n",
        "                    return ' '.join(palabras) # .join() limpia espacios m√∫ltiples\n",
        "\n",
        "                # 2. Convertir columnas a texto, rellenar vac√≠os y unir con un espacio\n",
        "                texto_unido = self.df[list(selected_cols)].fillna('').astype(str).apply(\n",
        "                    lambda fila: ' '.join(fila), axis=1\n",
        "                )\n",
        "\n",
        "                # 3. Acortar el texto ya unido\n",
        "                texto_acortado = texto_unido.apply(acortar_texto)\n",
        "\n",
        "                # 4. Crear el DataFrame final con una √∫nica columna llamada \"resumen\"\n",
        "                self.output_df = pd.DataFrame({'resumen': texto_acortado})\n",
        "\n",
        "                print(\"\\n‚úÖ ¬°Procesamiento completado con √©xito!\")\n",
        "\n",
        "                # --- L√≥gica de descarga ---\n",
        "                output_filename = 'resumen_concatenado.xlsx'\n",
        "                print(f\"\\nüì¶ Generando el archivo '{output_filename}' para descarga...\")\n",
        "\n",
        "                # Guardar el DataFrame en un archivo Excel dentro del entorno de Colab\n",
        "                self.output_df.to_excel(output_filename, index=False, engine='openpyxl')\n",
        "\n",
        "                # Usar la funci√≥n de Colab para iniciar la descarga en el navegador\n",
        "                files.download(output_filename)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\n‚ùå Ocurri√≥ un error inesperado durante el procesamiento.\")\n",
        "                print(f\"Detalle: {e}\")\n",
        "\n",
        "    def display_app(self):\n",
        "        \"\"\"Muestra la aplicaci√≥n completa en la celda.\"\"\"\n",
        "        display(\n",
        "            self.title,\n",
        "            self.instructions,\n",
        "            self.uploader,\n",
        "            self.step2_container,\n",
        "            self.output_area\n",
        "        )\n",
        "\n",
        "# --- Punto de entrada: Crear y mostrar la aplicaci√≥n ---\n",
        "app = VisualConcatenator()\n",
        "app.display_app()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "JkBDu3q3agWU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "outputId": "031d07fc-1cd3-4436-f614-adc4d00b36ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTML(value='<h2>üîó Concatenador Visual de Columnas</h2><hr>')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30c3a6a82e8947b68e39b201389b5354"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTML(value='<h4>Paso 1: Sube tu archivo Excel para empezar.</h4>')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "050b8f79ece24c38a828f8839ff66d6f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FileUpload(value={}, accept='.xlsx', button_style='primary', description='Subir Archivo')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "abecf80a7e6b467c97e6263e450a3a27"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "971cb6a17a44433eb724f098de536fdc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef831157903347d58c5f7d07a305961a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üîß Instalaciones\n",
        "\n",
        "# Optimizado para saturar la GPU y procesar miles de textos por segundo\n",
        "# Usa batch sizes grandes y procesamiento paralelo\n",
        "\n",
        "# ===============================================================\n",
        "# 1) Instalaci√≥n\n",
        "# ===============================================================\n",
        "print(\"Instalando dependencias...\")\n",
        "!pip install -q transformers sentence-transformers pandas openpyxl tqdm accelerate\n",
        "print(\"Listo.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "h491sbgkLMMv",
        "outputId": "b74d7a1c-4440-455f-c2b4-af5854b27c09"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instalando dependencias...\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mListo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Z3InbF8OaIqK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fd288aa5-a2d9-4c44-e495-05c6726661f7",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì GPU: Tesla T4\n",
            "‚úì VRAM Total: 14.7 GB\n",
            "‚úì Compute Capability: 7.5\n",
            "======================================================================\n",
            "  ANALIZADOR GPU-OPTIMIZED - M√ÅXIMA VELOCIDAD\n",
            "======================================================================\n",
            "‚ö° Batch Sentimiento: 2048 textos simult√°neos\n",
            "‚ö° Batch Embeddings: 4096 textos simult√°neos\n",
            "üî¨ Modelo Sentimiento: cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual\n",
            "üî¨ Modelo Temas: sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n",
            "======================================================================\n",
            "\n",
            "üì§ Suba su archivo Excel con columna 'resumen'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b0c9ba83-15a4-473f-93a9-2f24d858d4e4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b0c9ba83-15a4-473f-93a9-2f24d858d4e4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving resumen_concatenado (2).xlsx to resumen_concatenado (2) (3).xlsx\n",
            "\n",
            "üìä Leyendo: resumen_concatenado (2) (3).xlsx\n",
            "‚úì 5 textos cargados\n",
            "üîç Agrupando textos similares...\n",
            "‚úì 5 textos ‚Üí 5 grupos (0.0% reducci√≥n)\n",
            "\n",
            "======================================================================\n",
            "INICIANDO PROCESAMIENTO GPU\n",
            "======================================================================\n",
            "\n",
            "üéØ Analizando sentimiento con cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual\n",
            "   Batch size: 2048 | Max length: 128\n",
            "   GPU Memory: 0.54 GB allocated, 1.48 GB reserved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sentimiento:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c25151378d9c40a5baae4b1e82a6c210"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚ö° Procesados 5 textos en 0.50s (10 textos/s)\n",
            "   GPU Memory: 0.54 GB allocated, 1.48 GB reserved\n",
            "\n",
            "üè∑Ô∏è  Clasificando temas con sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n",
            "   Batch size: 4096\n",
            "   GPU Memory: 1.04 GB allocated, 1.12 GB reserved\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Embeddings:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e77c90ec6ffb4a0b8f5b3b5552da6511"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚ö° Procesados 5 textos en 0.08s (63 textos/s)\n",
            "   GPU Memory: 1.04 GB allocated, 1.14 GB reserved\n",
            "\n",
            "======================================================================\n",
            "‚úÖ COMPLETADO\n",
            "‚ö° Tiempo total: 14.08 segundos\n",
            "‚ö° Velocidad: 0 textos/segundo\n",
            "======================================================================\n",
            "\n",
            "üìä DISTRIBUCI√ìN:\n",
            "\n",
            "Tonos:\n",
            "tono\n",
            "Neutro      4\n",
            "Negativo    1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Temas principales:\n",
            "tema\n",
            "Salud P√∫blica y Sanidad          3\n",
            "Pol√≠ticas P√∫blicas y Gobierno    2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "üìù Generando Excel...\n",
            "üíæ Descargando: analisis_fast_resumen_concatenado__2___3__20251021_003548.xlsx\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d331678b-b761-4308-9b95-8d7ad2d26a1a\", \"analisis_fast_resumen_concatenado__2___3__20251021_003548.xlsx\", 8187)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Descarga completada\n",
            "\n",
            "‚úÖ Proceso completado\n"
          ]
        }
      ],
      "source": [
        "# @title üöÄ Analizador de Tono y Tema v13 GPU-OPTIMIZED ‚Äì M√°xima Velocidad\n",
        "\n",
        "# ===============================================================\n",
        "# 2) Importaciones\n",
        "# ===============================================================\n",
        "import os\n",
        "import io\n",
        "import re\n",
        "import gc\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================================================\n",
        "# 3) Configuraci√≥n GPU AGRESIVA\n",
        "# ===============================================================\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    # Optimizaciones agresivas para GPU\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Obtener info de GPU\n",
        "    gpu_props = torch.cuda.get_device_properties(0)\n",
        "    total_memory = gpu_props.total_memory / 1024**3\n",
        "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"‚úì VRAM Total: {total_memory:.1f} GB\")\n",
        "    print(f\"‚úì Compute Capability: {gpu_props.major}.{gpu_props.minor}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"‚ö†Ô∏è  CPU en uso. Active GPU (Runtime > Change runtime type > T4 GPU)\")\n",
        "    raise RuntimeError(\"GPU requerida para este notebook\")\n",
        "\n",
        "# ===============================================================\n",
        "# MODELOS - Los m√°s r√°pidos y precisos\n",
        "# ===============================================================\n",
        "\n",
        "# MODELO DE SENTIMIENTO - R√°pido y preciso\n",
        "MODELO_SENTIMIENTO = \"cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual\"\n",
        "\n",
        "# MODELO DE EMBEDDINGS - Versi√≥n base para balance velocidad/precisi√≥n\n",
        "MODELO_EMBEDDING = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
        "\n",
        "# PAR√ÅMETROS OPTIMIZADOS PARA GPU T4/V100 (15GB)\n",
        "# Ajustados para saturar la GPU sin OOM\n",
        "BATCH_SENT = 2048        # ¬°Enorme! Procesar√° miles de textos a la vez\n",
        "BATCH_EMB = 4096         # A√∫n m√°s grande para embeddings\n",
        "MAX_LEN_SENT = 128       # Balance entre contexto y velocidad\n",
        "MAX_CHARS_CLEAN = 400\n",
        "\n",
        "# Agrupamiento\n",
        "EXACT_PREFIX_LEN = 80\n",
        "MIN_PREFIX_LEN = 20\n",
        "\n",
        "# Workers para DataLoader\n",
        "NUM_WORKERS = 4\n",
        "\n",
        "# ===============================================================\n",
        "# 4) Diccionarios para temas\n",
        "# ===============================================================\n",
        "TOPIC_KEYWORDS = {\n",
        "    \"Pol√≠ticas P√∫blicas y Gobierno\": [\"reforma\",\"decreto\",\"ley\",\"plan nacional\",\"conpes\",\"congreso\",\"gobierno\",\"pol√≠tica\"],\n",
        "    \"Gesti√≥n Presupuestaria y Recursos\": [\"presupuesto\",\"licitaci√≥n\",\"contrato\",\"inversi√≥n\",\"vigencias\",\"adici√≥n\",\"recursos\"],\n",
        "    \"Salud P√∫blica y Sanidad\": [\"hospital\",\"vacunaci√≥n\",\"salud p√∫blica\",\"epidemia\",\"covid\",\"ips\",\"eps\",\"medicina\"],\n",
        "    \"Educaci√≥n y Desarrollo Acad√©mico\": [\"colegios\",\"universidades\",\"icfes\",\"becas\",\"educaci√≥n\",\"saber\",\"estudiantes\"],\n",
        "    \"Seguridad Ciudadana y Justicia\": [\"polic√≠a\",\"delito\",\"captura\",\"homicidio\",\"fiscal√≠a\",\"juzgado\",\"crimen\"],\n",
        "    \"Econom√≠a, Empleo y Desarrollo\": [\"pib\",\"inflaci√≥n\",\"empleo\",\"desempleo\",\"pymes\",\"exportaciones\",\"econom√≠a\"],\n",
        "    \"Infraestructura y Obras P√∫blicas\": [\"v√≠a\",\"doble calzada\",\"intercambiador\",\"vivienda\",\"energ√≠a\",\"metro\",\"obra\"],\n",
        "    \"Relaciones Exteriores y Cooperaci√≥n\": [\"canciller√≠a\",\"acuerdo bilateral\",\"cooperaci√≥n\",\"embajada\",\"internacional\"],\n",
        "    \"Medio Ambiente y Sostenibilidad\": [\"deforestaci√≥n\",\"emisiones\",\"renovables\",\"biodiversidad\",\"clima\",\"ambiente\"],\n",
        "    \"Transparencia y Participaci√≥n Ciudadana\": [\"veedur√≠a\",\"rendici√≥n de cuentas\",\"corrupci√≥n\",\"participaci√≥n\",\"transparencia\"],\n",
        "}\n",
        "TOPIC_LIST = list(TOPIC_KEYWORDS.keys())\n",
        "\n",
        "# ===============================================================\n",
        "# 5) Utilidades\n",
        "# ===============================================================\n",
        "\n",
        "def sanitize_name(name):\n",
        "    return re.sub(r\"[^A-Za-z0-9_\\-]\", \"_\", name)\n",
        "\n",
        "def clean_text_basic(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    t = text.strip().replace(\"\\n\", \" \")\n",
        "    t = re.sub(r\"[\\x00-\\x1f\\x7f]\", \" \", t)\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "    if len(t) > MAX_CHARS_CLEAN:\n",
        "        t = t[:MAX_CHARS_CLEAN].rsplit(\" \", 1)[0] if \" \" in t[:MAX_CHARS_CLEAN] else t[:MAX_CHARS_CLEAN]\n",
        "    return t\n",
        "\n",
        "def normalize_for_prefix(text):\n",
        "    t = text.lower().strip()\n",
        "    t = re.sub(r\"https?://\\S+|www\\.\\S+\", \" \", t)\n",
        "    t = re.sub(r\"[^\\w\\s√°√©√≠√≥√∫√º√±]\", \" \", t)\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t\n",
        "\n",
        "def print_gpu_usage():\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
        "        reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
        "        print(f\"   GPU Memory: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")\n",
        "\n",
        "# ===============================================================\n",
        "# 6) Agrupamiento r√°pido\n",
        "# ===============================================================\n",
        "\n",
        "def agrupar_por_prefijo(textos):\n",
        "    print(\"üîç Agrupando textos similares...\")\n",
        "    grupos = []\n",
        "    mapa = np.empty(len(textos), dtype=np.int32)\n",
        "    idx_por_pref = {}\n",
        "\n",
        "    for i, txt in enumerate(textos):\n",
        "        pref = normalize_for_prefix(txt)[:EXACT_PREFIX_LEN]\n",
        "        if len(pref) < MIN_PREFIX_LEN:\n",
        "            pref = f\"_short_{i}\"\n",
        "        gid = idx_por_pref.get(pref)\n",
        "        if gid is None:\n",
        "            gid = len(grupos)\n",
        "            idx_por_pref[pref] = gid\n",
        "            grupos.append([i])\n",
        "        else:\n",
        "            grupos[gid].append(i)\n",
        "        mapa[i] = gid\n",
        "\n",
        "    reps = []\n",
        "    for miembros in grupos:\n",
        "        rep_idx = max(miembros, key=lambda k: len(textos[k]))\n",
        "        reps.append(textos[rep_idx])\n",
        "\n",
        "    print(f\"‚úì {len(textos)} textos ‚Üí {len(reps)} grupos ({(1-len(reps)/len(textos))*100:.1f}% reducci√≥n)\")\n",
        "    return reps, mapa, grupos\n",
        "\n",
        "# ===============================================================\n",
        "# 7) Dataset y Sentimiento GPU-OPTIMIZADO\n",
        "# ===============================================================\n",
        "\n",
        "class TextDatasetFast(Dataset):\n",
        "    def __init__(self, textos, tokenizer, max_len):\n",
        "        self.textos = textos\n",
        "        self.tok = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.textos)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.tok(\n",
        "            self.textos[idx],\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            add_special_tokens=True,\n",
        "            return_tensors=None,\n",
        "        )\n",
        "\n",
        "@torch.inference_mode()\n",
        "def inferir_sentimiento_gpu_fast(reps):\n",
        "    print(f\"\\nüéØ Analizando sentimiento con {MODELO_SENTIMIENTO}\")\n",
        "    print(f\"   Batch size: {BATCH_SENT} | Max length: {MAX_LEN_SENT}\")\n",
        "\n",
        "    # Cargar modelo\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODELO_SENTIMIENTO)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODELO_SENTIMIENTO)\n",
        "\n",
        "    # Optimizaciones GPU\n",
        "    model = model.to(device)\n",
        "    model.half()  # FP16 para 2x m√°s r√°pido\n",
        "    model.eval()\n",
        "\n",
        "    print_gpu_usage()\n",
        "\n",
        "    # Dataset\n",
        "    dataset = TextDatasetFast(reps, tokenizer, MAX_LEN_SENT)\n",
        "    collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=BATCH_SENT,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        collate_fn=collator,\n",
        "        prefetch_factor=2\n",
        "    )\n",
        "\n",
        "    # Inferencia\n",
        "    all_preds = []\n",
        "    t_start = time.time()\n",
        "\n",
        "    with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):  # Mixed precision\n",
        "        for batch in tqdm(dataloader, desc=\"Sentimiento\"):\n",
        "            batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
        "            logits = model(**batch).logits\n",
        "            preds = logits.argmax(dim=-1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "\n",
        "    t_elapsed = time.time() - t_start\n",
        "    print(f\"   ‚ö° Procesados {len(reps)} textos en {t_elapsed:.2f}s ({len(reps)/t_elapsed:.0f} textos/s)\")\n",
        "    print_gpu_usage()\n",
        "\n",
        "    # Mapear labels (XLM-RoBERTa: 0=neg, 1=neu, 2=pos)\n",
        "    label_map = {0: \"Negativo\", 1: \"Neutro\", 2: \"Positivo\"}\n",
        "    sentimientos = [label_map[int(p)] for p in all_preds]\n",
        "\n",
        "    del model, tokenizer\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return sentimientos\n",
        "\n",
        "# ===============================================================\n",
        "# 8) Temas GPU-OPTIMIZADO\n",
        "# ===============================================================\n",
        "\n",
        "def inferir_temas_gpu_fast(reps):\n",
        "    print(f\"\\nüè∑Ô∏è  Clasificando temas con {MODELO_EMBEDDING}\")\n",
        "    print(f\"   Batch size: {BATCH_EMB}\")\n",
        "\n",
        "    # Cargar encoder con optimizaciones\n",
        "    encoder = SentenceTransformer(MODELO_EMBEDDING, device=device)\n",
        "    encoder.max_seq_length = 128  # Limitar para velocidad\n",
        "\n",
        "    print_gpu_usage()\n",
        "\n",
        "    # Preparar centroides\n",
        "    frases = []\n",
        "    for tema, kws in TOPIC_KEYWORDS.items():\n",
        "        frases.append(f\"passage: {tema}. \" + \" \".join(kws[:5]))\n",
        "\n",
        "    centroides = encoder.encode(\n",
        "        frases,\n",
        "        batch_size=32,\n",
        "        show_progress_bar=False,\n",
        "        convert_to_tensor=True,\n",
        "        normalize_embeddings=True,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Procesar textos en batches grandes\n",
        "    all_vecs = []\n",
        "    texts_prefixed = [f\"passage: {t}\" for t in reps]\n",
        "\n",
        "    t_start = time.time()\n",
        "    for i in tqdm(range(0, len(texts_prefixed), BATCH_EMB), desc=\"Embeddings\"):\n",
        "        batch = texts_prefixed[i:i+BATCH_EMB]\n",
        "        vecs = encoder.encode(\n",
        "            batch,\n",
        "            batch_size=512,  # Sub-batch interno\n",
        "            show_progress_bar=False,\n",
        "            convert_to_tensor=True,\n",
        "            normalize_embeddings=True,\n",
        "            device=device\n",
        "        )\n",
        "        all_vecs.append(vecs)\n",
        "\n",
        "    # Concatenar y calcular similitudes en GPU\n",
        "    all_vecs = torch.cat(all_vecs, dim=0)\n",
        "    similarities = torch.matmul(all_vecs, centroides.T)\n",
        "    tema_ids = similarities.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "    t_elapsed = time.time() - t_start\n",
        "    print(f\"   ‚ö° Procesados {len(reps)} textos en {t_elapsed:.2f}s ({len(reps)/t_elapsed:.0f} textos/s)\")\n",
        "    print_gpu_usage()\n",
        "\n",
        "    temas = [TOPIC_LIST[int(idx)] for idx in tema_ids]\n",
        "\n",
        "    del encoder, centroides, all_vecs, similarities\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return temas\n",
        "\n",
        "# ===============================================================\n",
        "# 9) Guardado\n",
        "# ===============================================================\n",
        "\n",
        "def guardar_resultados(df, nombre_original, grupos, t_total):\n",
        "    base = re.sub(r\"\\.xlsx?$\", \"\", nombre_original, flags=re.IGNORECASE)\n",
        "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    out_xlsx = f\"/content/analisis_fast_{sanitize_name(base)}_{ts}.xlsx\"\n",
        "\n",
        "    n = len(df)\n",
        "    n_grupos = len(grupos)\n",
        "\n",
        "    print(\"\\nüìù Generando Excel...\")\n",
        "\n",
        "    dist_tono = df['tono'].value_counts()\n",
        "    dist_tema = df['tema'].value_counts()\n",
        "\n",
        "    with pd.ExcelWriter(out_xlsx, engine=\"openpyxl\") as w:\n",
        "        cols = [\"resumen\", \"tono\", \"tema\"] + [c for c in df.columns if c not in [\"resumen\", \"tono\", \"tema\"]]\n",
        "        df[cols].to_excel(w, sheet_name=\"Resultados\", index=False)\n",
        "\n",
        "        stats = pd.DataFrame({\n",
        "            \"M√©trica\": [\n",
        "                \"Total de textos\",\n",
        "                \"Grupos procesados\",\n",
        "                \"Reducci√≥n\",\n",
        "                \"Tiempo total (s)\",\n",
        "                \"Velocidad (textos/s)\",\n",
        "                \"Batch Sentimiento\",\n",
        "                \"Batch Embeddings\",\n",
        "                \"Modelo Sentimiento\",\n",
        "                \"Modelo Temas\",\n",
        "            ],\n",
        "            \"Valor\": [\n",
        "                n,\n",
        "                n_grupos,\n",
        "                f\"{(1 - n_grupos/max(n,1))*100:.1f}%\",\n",
        "                f\"{t_total:.2f}\",\n",
        "                f\"{n/max(t_total,1):.0f}\",\n",
        "                BATCH_SENT,\n",
        "                BATCH_EMB,\n",
        "                MODELO_SENTIMIENTO,\n",
        "                MODELO_EMBEDDING,\n",
        "            ],\n",
        "        })\n",
        "        stats.to_excel(w, sheet_name=\"Estadisticas\", index=False)\n",
        "\n",
        "        dist_tono_df = pd.DataFrame({\n",
        "            'Tono': dist_tono.index,\n",
        "            'Cantidad': dist_tono.values,\n",
        "            'Porcentaje': (dist_tono.values / n * 100).round(2)\n",
        "        })\n",
        "        dist_tono_df.to_excel(w, sheet_name=\"Distribucion_Tonos\", index=False)\n",
        "\n",
        "        dist_tema_df = pd.DataFrame({\n",
        "            'Tema': dist_tema.index,\n",
        "            'Cantidad': dist_tema.values,\n",
        "            'Porcentaje': (dist_tema.values / n * 100).round(2)\n",
        "        })\n",
        "        dist_tema_df.to_excel(w, sheet_name=\"Distribucion_Temas\", index=False)\n",
        "\n",
        "    if os.path.exists(out_xlsx):\n",
        "        print(f\"üíæ Descargando: {os.path.basename(out_xlsx)}\")\n",
        "        try:\n",
        "            files.download(out_xlsx)\n",
        "            print(f\"‚úÖ Descarga completada\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Error: {e}\")\n",
        "            print(f\"üìÅ Archivo en: {out_xlsx}\")\n",
        "\n",
        "# ===============================================================\n",
        "# 10) Flujo principal\n",
        "# ===============================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\"*70)\n",
        "    print(\"  ANALIZADOR GPU-OPTIMIZED - M√ÅXIMA VELOCIDAD\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"‚ö° Batch Sentimiento: {BATCH_SENT} textos simult√°neos\")\n",
        "    print(f\"‚ö° Batch Embeddings: {BATCH_EMB} textos simult√°neos\")\n",
        "    print(f\"üî¨ Modelo Sentimiento: {MODELO_SENTIMIENTO}\")\n",
        "    print(f\"üî¨ Modelo Temas: {MODELO_EMBEDDING}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(\"\\nüì§ Suba su archivo Excel con columna 'resumen'\")\n",
        "    up = files.upload()\n",
        "    if not up:\n",
        "        print(\"‚ùå No se subi√≥ archivo\")\n",
        "        return\n",
        "\n",
        "    nombre = list(up.keys())[0]\n",
        "    print(f\"\\nüìä Leyendo: {nombre}\")\n",
        "    df = pd.read_excel(io.BytesIO(up[nombre]))\n",
        "\n",
        "    if 'resumen' not in df.columns:\n",
        "        print(f\"‚ùå Error: Falta columna 'resumen'. Columnas: {', '.join(df.columns)}\")\n",
        "        return\n",
        "\n",
        "    print(f\"‚úì {len(df)} textos cargados\")\n",
        "    textos = df['resumen'].fillna('').astype(str).map(clean_text_basic).tolist()\n",
        "    nombre_excel_base = sanitize_name(re.sub(r\"\\.xlsx?$\", \"\", nombre, flags=re.IGNORECASE))\n",
        "\n",
        "    # Agrupamiento\n",
        "    reps, mapa, grupos = agrupar_por_prefijo(textos)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"INICIANDO PROCESAMIENTO GPU\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Sentimiento\n",
        "    tonos_rep = inferir_sentimiento_gpu_fast(reps)\n",
        "\n",
        "    # Temas\n",
        "    temas_rep = inferir_temas_gpu_fast(reps)\n",
        "\n",
        "    # Propagar\n",
        "    n = len(df)\n",
        "    df['tono'] = [tonos_rep[int(mapa[i])] for i in range(n)]\n",
        "    df['tema'] = [temas_rep[int(mapa[i])] for i in range(n)]\n",
        "\n",
        "    t1 = time.time()\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"‚úÖ COMPLETADO\")\n",
        "    print(f\"‚ö° Tiempo total: {t1 - t0:.2f} segundos\")\n",
        "    print(f\"‚ö° Velocidad: {n/max(1e-6, t1 - t0):.0f} textos/segundo\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    print(\"\\nüìä DISTRIBUCI√ìN:\")\n",
        "    print(f\"\\nTonos:\\n{df['tono'].value_counts()}\")\n",
        "    print(f\"\\nTemas principales:\\n{df['tema'].value_counts().head(5)}\")\n",
        "\n",
        "    guardar_resultados(df, nombre_excel_base, grupos, t1 - t0)\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"\\n‚úÖ Proceso completado\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úç Explicaci√≥n"
      ],
      "metadata": {
        "id": "cvGZHrX66Rv7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c91fe41d"
      },
      "source": [
        "Este c√≥digo es un \"Analizador de Tono y Tema v13 GPU-OPTIMIZED ‚Äì M√°xima Velocidad üöÄ\" dise√±ado para procesar textos en espa√±ol, determinar su sentimiento (positivo, negativo o neutro) y clasificarlos por tema.\n",
        "\n",
        "### 1. Instalaci√≥n e Importaciones\n",
        "\n",
        "* `!pip install -q ...`: Instala las librer√≠as necesarias para que el c√≥digo funcione. Las m√°s importantes son:\n",
        "  * `transformers`: Permite usar modelos de lenguaje pre-entrenados (como los que analizan sentimiento).\n",
        "  * `sentence-transformers`: Sirve para convertir texto en \"embeddings\" (representaciones num√©ricas que capturan el significado) y encontrar textos similares o relacionados.\n",
        "  * `pandas`: Esencial para manejar datos en formato de tablas (como hojas de c√°lculo de Excel).\n",
        "  * `openpyxl`: Necesario para leer y escribir archivos Excel (`.xlsx`).\n",
        "  * `tqdm`: Muestra barras de progreso para que sepas cu√°nto falta en procesos largos.\n",
        "  * `accelerate`: Ayuda a optimizar el uso de hardware (como la GPU) para que los modelos sean m√°s r√°pidos.\n",
        "* `import ...`: Carga las librer√≠as y funciones que se usar√°n en el c√≥digo.\n",
        "\n",
        "### 2. Configuraci√≥n\n",
        "\n",
        "* Establece par√°metros como la semilla aleatoria (`SEED`) para que los resultados sean reproducibles, detecta si tienes una GPU (tarjeta gr√°fica) disponible para acelerar el procesamiento y define qu√© modelos de lenguaje r√°pidos se van a usar (`MODELO_SENTIMIENTO` y `MODELO_EMBEDDING`).\n",
        "* Tambi√©n ajusta par√°metros de velocidad como el tama√±o de los lotes (`BATCH_SENT`, `BATCH_EMB`) y la longitud m√°xima de texto a procesar (`MAX_LEN_SENT`, `MAX_CHARS_CLEAN`).\n",
        "\n",
        "### 3. Diccionarios para temas\n",
        "\n",
        "* Define palabras clave por tema (`TOPIC_KEYWORDS`). Estos se usan para clasificar los textos seg√∫n su similitud con estos t√©rminos.\n",
        "\n",
        "### 4. Utilidades\n",
        "\n",
        "* Son peque√±as funciones de ayuda:\n",
        "  * `sanitize_name`: Limpia nombres de archivo para que no tengan caracteres extra√±os.\n",
        "  * `clean_text_basic`: Limpia el texto b√°sico (quita saltos de l√≠nea, espacios extra, etc.) y lo recorta si es muy largo.\n",
        "  * `normalize_for_prefix`: Prepara el texto para el agrupamiento, quitando URLs y puntuaci√≥n, y pas√°ndolo a min√∫sculas.\n",
        "  * `print_gpu_usage`: Muestra el uso actual de la memoria de la GPU.\n",
        "\n",
        "### 5. Agrupamiento r√°pido\n",
        "\n",
        "* `agrupar_por_prefijo`: Es una funci√≥n clave para la velocidad. Identifica textos que empiezan exactamente igual en una cierta longitud y los agrupa. Esto permite procesar solo un representante por cada grupo en lugar de cada texto individual, ahorrando mucho tiempo.\n",
        "\n",
        "### 6. Sentimiento GPU-OPTIMIZADO\n",
        "\n",
        "* Usa el modelo `cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual` para predecir el sentimiento de cada texto representante. Este modelo es **open source** y se distribuye bajo la licencia **Apache 2.0**, lo que generalmente permite su uso comercial.\n",
        "\n",
        "### 7. Temas GPU-OPTIMIZADO\n",
        "\n",
        "* Usa el modelo `sentence-transformers/paraphrase-multilingual-mpnet-base-v2` para convertir los textos representantes en \"embeddings\" (vectores num√©ricos). Luego compara estos vectores con los vectores promedio de las palabras clave de cada tema para encontrar el tema m√°s similar. Este modelo tambi√©n es **open source** y se basa en modelos con licencia **Apache 2.0**, permitiendo su uso comercial.\n",
        "\n",
        "### 8. Guardado\n",
        "\n",
        "* `guardar_resultados`: Toma los resultados del an√°lisis (sentimiento y tema para cada texto), los a√±ade al DataFrame original y guarda todo en un archivo Excel. Tambi√©n genera hojas con estad√≠sticas del procesamiento y distribuciones de tonos y temas.\n",
        "\n",
        "### 9. Flujo principal (`main`)\n",
        "\n",
        "* Esta es la funci√≥n que orquesta todo.\n",
        "* Te pide que subas un archivo Excel con una columna llamada \"resumen\".\n",
        "* Limpia los textos de la columna \"resumen\".\n",
        "* Agrupa los textos por prefijo para identificar duplicados.\n",
        "* Realiza el an√°lisis de sentimiento y tema sobre los textos representantes (los √∫nicos de cada grupo) utilizando las funciones optimizadas para GPU.\n",
        "* Propaga los resultados (sentimiento y tema) a todos los textos originales (usando el mapa de agrupamiento).\n",
        "* Calcula y muestra el tiempo total que tard√≥ el procesamiento y la velocidad.\n",
        "* Muestra las distribuciones de tonos y temas encontrados.\n",
        "* Guarda el archivo Excel final con los resultados del an√°lisis y las estad√≠sticas, y lo descarga.\n",
        "\n",
        "En resumen, el c√≥digo optimiza el an√°lisis de grandes vol√∫menes de texto en espa√±ol usando modelos de lenguaje r√°pidos y open source (con licencias que permiten uso comercial) y una t√©cnica de agrupamiento por prefijo para evitar procesar textos id√©nticos o muy similares varias veces."
      ]
    }
  ]
}