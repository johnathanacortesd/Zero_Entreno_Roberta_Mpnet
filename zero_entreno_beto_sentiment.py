# -*- coding: utf-8 -*-
"""Zero_Entreno_beto-sentiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vFgoU5bkl3OLJ7PIjQZlSEznJdfDANnS
"""

# @title üìä Interfaz para Concatenar Columnas
# --- Importaciones necesarias ---
import pandas as pd
import ipywidgets as widgets
from google.colab import files
from IPython.display import display, clear_output
import io

# --- Instalaci√≥n silenciosa de la dependencia para .xlsx ---
!pip install -q openpyxl

# --- Clase principal de la aplicaci√≥n ---
class VisualConcatenator:
    def __init__(self):
        """Inicializa la interfaz y sus componentes."""
        self.df = None
        self.output_df = None

        # --- Definici√≥n de los Widgets (Componentes de la UI) ---
        self.title = widgets.HTML("<h2>üîó Concatenador Visual de Columnas</h2><hr>")
        self.instructions = widgets.HTML("<h4>Paso 1: Sube tu archivo Excel para empezar.</h4>")

        self.uploader = widgets.FileUpload(
            accept='.xlsx',
            description='Subir Archivo',
            button_style='primary'
        )

        # Contenedor para la segunda parte de la UI (que aparece despu√©s de subir)
        self.step2_container = widgets.VBox([])

        # √Årea de salida para mensajes y resultados
        self.output_area = widgets.Output()

        # --- Observador de eventos ---
        # Llama a la funci√≥n _handle_upload cuando se sube un archivo
        self.uploader.observe(self._handle_upload, names='value')

    def _handle_upload(self, change):
        """Se activa al subir un archivo. Lee el archivo y crea la UI de selecci√≥n."""
        # Limpia cualquier contenido previo en el √°rea de salida
        with self.output_area:
            clear_output()

        uploaded_file_info = self.uploader.value
        if not uploaded_file_info:
            return # Si se cancela la subida, no hacer nada

        # Obtener el nombre y contenido del archivo subido
        filename = list(uploaded_file_info.keys())[0]
        content = uploaded_file_info[filename]['content']

        try:
            with self.output_area:
                print(f"‚è≥ Leyendo '{filename}'...")
                # Leer el archivo Excel desde la memoria
                self.df = pd.read_excel(io.BytesIO(content))
                print(f"‚úÖ ¬°Archivo cargado! Se encontraron {self.df.shape[0]} filas.")
                print(f"üìä Columnas disponibles: {', '.join(self.df.columns)}")

            # Si el archivo se ley√≥ bien, crear la siguiente parte de la interfaz
            self._create_column_selector_ui()

        except Exception as e:
            with self.output_area:
                print(f"‚ùå Error al leer el archivo. Aseg√∫rate de que es un .xlsx v√°lido.")
                print(f"Detalle: {e}")

    def _create_column_selector_ui(self):
        """Crea los widgets para seleccionar columnas y procesar."""

        # Nuevas instrucciones para el usuario
        step2_instructions = widgets.HTML("""
        <h4>Paso 2: Selecciona las columnas a unir.</h4>
        <p><i>Usa <b>Ctrl+Click</b> (o <b>Cmd+Click</b> en Mac) para elegir varias.</i></p>
        """)

        # Widget para seleccionar m√∫ltiples columnas
        self.column_selector = widgets.SelectMultiple(
            options=self.df.columns.tolist(),
            description='Columnas:',
            layout=widgets.Layout(height='180px', width='95%'),
            style={'description_width': 'initial'}
        )

        # Bot√≥n para iniciar el procesamiento
        self.process_button = widgets.Button(
            description='üîó Procesar y Descargar Resultado',
            button_style='success',
            icon='cogs',
            layout=widgets.Layout(width='300px', height='40px', margin='10px 0 0 0')
        )

        # Conectar el bot√≥n a la funci√≥n de procesamiento
        self.process_button.on_click(self._process_and_download)

        # Actualizar el contenedor del Paso 2 con los nuevos widgets
        self.step2_container.children = [
            step2_instructions,
            self.column_selector,
            self.process_button
        ]

    def _process_and_download(self, b):
        """Realiza la concatenaci√≥n, acortado y descarga del archivo."""
        with self.output_area:
            clear_output()

            selected_cols = self.column_selector.value
            if len(selected_cols) < 2:
                print("‚ùå Error: Debes seleccionar al menos dos columnas para poder unirlas.")
                return

            print("üöÄ Procesando los datos...")
            print(f"   - Uniendo columnas: {', '.join(selected_cols)}")

            try:
                # --- L√≥gica principal ---
                # 1. Funci√≥n para acortar el texto a 80 palabras
                def acortar_texto(texto, limite=80):
                    palabras = str(texto).split()
                    if len(palabras) > limite:
                        return ' '.join(palabras[:limite]) + '...'
                    return ' '.join(palabras) # .join() limpia espacios m√∫ltiples

                # 2. Convertir columnas a texto, rellenar vac√≠os y unir con un espacio
                texto_unido = self.df[list(selected_cols)].fillna('').astype(str).apply(
                    lambda fila: ' '.join(fila), axis=1
                )

                # 3. Acortar el texto ya unido
                texto_acortado = texto_unido.apply(acortar_texto)

                # 4. Crear el DataFrame final con una √∫nica columna llamada "resumen"
                self.output_df = pd.DataFrame({'resumen': texto_acortado})

                print("\n‚úÖ ¬°Procesamiento completado con √©xito!")

                # --- L√≥gica de descarga ---
                output_filename = 'resumen_concatenado.xlsx'
                print(f"\nüì¶ Generando el archivo '{output_filename}' para descarga...")

                # Guardar el DataFrame en un archivo Excel dentro del entorno de Colab
                self.output_df.to_excel(output_filename, index=False, engine='openpyxl')

                # Usar la funci√≥n de Colab para iniciar la descarga en el navegador
                files.download(output_filename)

            except Exception as e:
                print(f"\n‚ùå Ocurri√≥ un error inesperado durante el procesamiento.")
                print(f"Detalle: {e}")

    def display_app(self):
        """Muestra la aplicaci√≥n completa en la celda."""
        display(
            self.title,
            self.instructions,
            self.uploader,
            self.step2_container,
            self.output_area
        )

# --- Punto de entrada: Crear y mostrar la aplicaci√≥n ---
app = VisualConcatenator()
app.display_app()

"""# Analizador v2.0 H√çBRIDO
## ML (E5) para clustering sem√°ntico ‚Üí Agrupa similares
##üìè Reglas ling√º√≠sticas para tono ‚Üí Explicable y controlable
##üè∑Ô∏è Keywords para tema ‚Üí Adaptable al dominio
"""

# @title üîß Instalaciones
!pip install -q torch transformers sentence-transformers pandas openpyxl tqdm scikit-learn 2>/dev/null

# @title Analizador v2.0

# @markdown ‚Ä¢ An√°lisis de TONO
# @markdown ‚Ä¢ NO usa modelo de ML - Usa reglas ling√º√≠sticas

import os, re, time, torch
import numpy as np
import pandas as pd
import warnings
from tqdm.auto import tqdm
from sentence_transformers import SentenceTransformer
from collections import defaultdict
from google.colab import files

warnings.filterwarnings("ignore")

# ==================== CONFIGURACI√ìN ====================
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"üöÄ Dispositivo: {device.upper()}")

# Modelo multiling√ºe con licencia Apache 2.0
embedder = SentenceTransformer("intfloat/multilingual-e5-large", device=device)

# ==================== REGLAS MEJORADAS ====================
POSITIVE_VERBS_CLIENT = [
    "confirma", "anuncia", "inaugura", "lanza", "lidera", "impulsa", "acompa√±a","continua",
    "gestiona", "acompa√±a", "otorga", "destina", "facilita", "apoya", "confirma","recibe",
    "reconoce", "homenajea", "firma", "inicia", "invierte", "entrega", "adelanta","demuestra",
    "presenta", "logra", "avanza", "atiende", "visita", "verifica","contin√∫a","intensifica",
    "construye", "desarrolla", "fortalece", "promueve", "garantiza","expresa",
]

# Contextos que indican gesti√≥n POSITIVA del cliente
POSITIVE_CONTEXTS = [
    "todo est√° listo", "obras de", "inversi√≥n en","recibe distinci√≥n",
    "mejoras en", "avance en", "beneficio", "articulaci√≥n","expresa solidaridad",
    "liderazgo del", "bajo el liderazgo", "gracias a la alcald√≠a",
    "gracias al alcalde", "proyecto es posible gracias",
    "gesti√≥n de", "gesti√≥n del alcalde"
]

# Logros de programas/proyectos que implican gesti√≥n positiva
ACHIEVEMENT_INDICATORS = [
    "bachilleres inician", "bachilleres se han inscrito", "estudiantes inician",
    "familias beneficiadas", "viviendas entregadas", "obras culminadas",
    "proyectos en marcha", "inversi√≥n de", "beneficiarios",
    "acceso a", "programas profesionales", "vida universitaria"
]

# Palabras que indican acciones REACTIVAS (neutras, no gesti√≥n)
NEUTRAL_REACTIONS = [
    "lamenta", "rechaza", "condena", "expresa solidaridad",
    "pide", "solicita", "exige", "manifesta preocupaci√≥n"
]

# Contextos claramente negativos
NEGATIVE_TRIGGERS = [
    "esc√°ndalo", "corrupci√≥n", "investigan al", "denuncian al",
    "irregularidad", "desfalco", "acusan", "protesta contra",
    "demanda contra", "sancionan", "fraude", "malversaci√≥n"
]

TOPICS_KEYWORDS = {
    "Infraestructura": ["escenario", "multideportivo", "obra", "infraestructura", "v√≠a", "parque",
                        "pavimentaci√≥n", "construcci√≥n", "puente", "polideportivo", "cancha"],
    "Educaci√≥n": ["educaci√≥n", "bachilleres", "universitaria", "colegio", "becas", "estudiantes",
                  "escuela", "universidad", "educaci√≥n superior", "acceso a la universidad"],
    "Seguridad y Justicia": ["polic√≠a", "seguridad", "auxiliares", "dotaci√≥n", "homicidio",
                            "delincuencia", "crimen", "atentado", "fallecimiento"],
    "Relaciones Internacionales": ["embajador", "cumbre", "celac", "diplom√°tico", "internacional", "bilateral"],
    "Corrupci√≥n y Esc√°ndalos": ["esc√°ndalo", "corrupci√≥n", "investigan al", "denuncia contra", "fraude"],
    "Salud": ["salud", "hospital", "m√©dico", "atenci√≥n m√©dica", "vacunaci√≥n", "pandemia"],
    "Medio Ambiente": ["lluvias", "inundaciones", "emergencia", "clima", "ambiental", "desastre natural",
                      "hurac√°n", "afectados", "coletazo"],
    "Econom√≠a": ["econom√≠a", "empleo", "empresa", "comercio", "inversi√≥n econ√≥mica", "presupuesto"],
}

# ==================== FUNCIONES CORREGIDAS ====================

def normalize_text(text):
    """Normaliza texto removiendo art√≠culos y preposiciones para mejor clustering"""
    # Convertir a min√∫sculas
    t = text.lower()

    # Remover art√≠culos y preposiciones comunes
    stopwords = ['el', 'la', 'los', 'las', 'un', 'una', 'unos', 'unas',
                 'de', 'del', 'en', 'para', 'por', 'con', 'sin', 'sobre',
                 'que', 'todo', 'est√°', 'son', 'es']

    # Mantener solo caracteres alfanum√©ricos y espacios
    t = re.sub(r'[^\w√°√©√≠√≥√∫√±\s]', ' ', t)
    t = re.sub(r'\s+', ' ', t).strip()

    # Remover stopwords
    words = t.split()
    words = [w for w in words if w not in stopwords and len(w) > 2]

    return " ".join(words)

def get_signature(text, words=20):
    """Genera firma textual normalizada para clustering"""
    normalized = normalize_text(text)
    # Tomar m√°s palabras para capturar mejor la similitud
    return " ".join(normalized.split()[:words])

def universal_cluster(texts):
    """Clustering REAL - noticias similares tendr√°n mismo tema/tono"""
    print("üìä Agrupando noticias similares...")
    groups = defaultdict(list)

    for i, t in enumerate(texts):
        signature = get_signature(t)
        groups[signature].append(i)

    representatives = []
    cluster_map = [-1] * len(texts)
    cid = 0

    for idxs in groups.values():
        # El representante es el texto m√°s largo del grupo (m√°s completo)
        longest_idx = max(idxs, key=lambda i: len(texts[i]))
        representatives.append(texts[longest_idx])

        for i in idxs:
            cluster_map[i] = cid
        cid += 1

    print(f"‚úÖ {len(texts)} noticias ‚Üí {len(representatives)} grupos √∫nicos")
    return representatives, np.array(cluster_map)

def analyze_universal(rep_text, client_name):
    """An√°lisis MEJORADO con l√≥gica de contexto"""
    lower = rep_text.lower()
    client_lower = client_name.lower()

    # === AN√ÅLISIS DE TONO CON PRIORIDADES ===

    # PRIORIDAD 1: Negativos directos contra el cliente
    negative_about_client = any(
        f"{neg} {client_lower}" in lower or f"{client_lower} {neg}" in lower
        for neg in ["investigan al", "denuncian al", "acusan", "esc√°ndalo", "corrupci√≥n"]
    )

    if negative_about_client:
        tono = "Negativo"
        confianza = 0.95

    # PRIORIDAD 2: Reacciones neutras (lamenta, rechaza eventos externos)
    elif any(reaction in lower for reaction in NEUTRAL_REACTIONS):
        # Si solo reacciona a eventos externos = Neutro
        # (no es gesti√≥n propia, solo declaraciones)
        tono = "Neutro"
        confianza = 0.88

    # PRIORIDAD 3: Gesti√≥n positiva del cliente
    else:
        # Buscar si el cliente ES EL GESTOR (sujeto activo)
        is_active_subject = False

        # Contexto 1: "liderazgo del alcalde X", "gracias a X"
        leadership_patterns = [
            f"liderazgo del {client_lower}",
            f"liderazgo de {client_lower}",
            f"gracias a {client_lower}",
            f"gracias al {client_lower}",
            f"gesti√≥n del {client_lower}",
            f"gesti√≥n de {client_lower}"
        ]

        if any(pattern in lower for pattern in leadership_patterns):
            is_active_subject = True

        # Contexto 2: "alcalde X + verbo de gesti√≥n activa"
        if not is_active_subject:
            active_verbs = ["confirma", "anuncia", "inaugura", "entrega", "invierte",
                          "construye", "lidera", "impulsa", "gestiona", "inicia"]

            for verb in active_verbs:
                # Patr√≥n: "alcalde X verbo"
                if f"alcalde {client_lower} {verb}" in lower or \
                   f"alcald√≠a {verb}" in lower:
                    is_active_subject = True
                    break

        # Contexto 3: Logros/resultados de programas (implican gesti√≥n)
        # "500 bachilleres inician...", "familias beneficiadas..."
        has_achievement = any(indicator in lower for indicator in ACHIEVEMENT_INDICATORS)

        # Contexto 4: Presencia de indicadores positivos generales
        has_positive_context = any(ctx in lower for ctx in POSITIVE_CONTEXTS)

        # Si hay logros concretos en √°reas de gesti√≥n p√∫blica = Positivo
        if is_active_subject or has_positive_context or has_achievement:
            tono = "Positivo"
            confianza = 0.92 if is_active_subject else 0.85
        else:
            tono = "Neutro"
            confianza = 0.70

    # === AN√ÅLISIS DE TEMA ===
    tema = "Gesti√≥n y Acciones"  # Por defecto
    max_matches = 0

    for topic, keywords in TOPICS_KEYWORDS.items():
        matches = sum(1 for kw in keywords if kw in lower)
        if matches > max_matches:
            max_matches = matches
            tema = topic

    return tono, round(confianza, 3), tema

# ==================== MAIN EXECUTION ====================
print("\n" + "="*80)
print("üì∞ ANALIZADOR DE NOTICIAS v30 - CLUSTERING REAL")
print("="*80 + "\n")

# Solicitar nombre del cliente
client_name = input("üë§ Nombre del cliente (ej: Carlos Pinedo): ").strip()
if not client_name:
    client_name = "Carlos Pinedo"
    print(f"   Usando cliente por defecto: {client_name}")

# Cargar archivo
print("\nüìÇ Sube tu archivo Excel...")
uploaded = files.upload()
file_name = list(uploaded.keys())[0]

# Leer datos
print(f"üìñ Leyendo {file_name}...")
df = pd.read_excel(file_name)

# Detectar columna de texto
text_columns = ['resumen', 'texto', 'contenido', 'titulo', 'title', 'noticia', 'descripcion']
text_col = next((c for c in text_columns if c in df.columns), None)

if not text_col:
    print(f"‚ùå Error: No se encontr√≥ columna de texto. Columnas disponibles: {list(df.columns)}")
    raise ValueError("No se encontr√≥ columna de texto v√°lida")

print(f"‚úÖ Columna detectada: '{text_col}'")

# Limpiar datos
df = df.dropna(subset=[text_col]).reset_index(drop=True)
texts = df[text_col].astype(str).tolist()
print(f"üìä {len(texts)} noticias cargadas\n")

# ==================== PROCESAMIENTO ====================
t0 = time.time()

# Clustering REAL
rep_texts, cluster_map = universal_cluster(texts)

# An√°lisis - CADA GRUPO SE ANALIZA UNA SOLA VEZ
print("\nüîç Analizando tono y tema de grupos √∫nicos...")
tonos = []
confianzas = []
temas = []

for rep in tqdm(rep_texts, desc="Procesando"):
    tono, conf, tema = analyze_universal(rep, client_name)
    tonos.append(tono)
    confianzas.append(conf)
    temas.append(tema)

# Mapear resultados - TODAS las noticias del mismo grupo tendr√°n EXACTAMENTE el mismo tema/tono
print("\nüîó Aplicando tema/tono a grupos similares...")
df["grupo_id"] = cluster_map
df["tono_marca"] = [tonos[cluster_map[i]] for i in range(len(df))]
df["confianza"] = [confianzas[cluster_map[i]] for i in range(len(df))]
df["tema"] = [temas[cluster_map[i]] for i in range(len(df))]

# Verificaci√≥n: Mostrar algunos grupos para debugging
print("\nüîç Verificando grupos (muestra):")
for gid in sorted(df["grupo_id"].unique())[:3]:
    grupo = df[df["grupo_id"] == gid]
    if len(grupo) > 1:
        print(f"\n   Grupo {gid} ({len(grupo)} noticias) - Tono: {tonos[gid]} - Tema: {temas[gid]}")
        for idx, row in grupo.head(2).iterrows():
            texto_corto = row[text_col][:80] + "..." if len(row[text_col]) > 80 else row[text_col]
            print(f"      ‚Ä¢ {texto_corto}")

# ==================== EXPORTACI√ìN (SOLO 1 HOJA) ====================
out = f"Final_{client_name.replace(' ', '_')}_{time.strftime('%Y%m%d_%H%M')}.xlsx"
print(f"\nüíæ Guardando resultados en {out}...")

# SOLO UNA HOJA con todos los resultados
df.to_excel(out, sheet_name="Resultados", index=False, engine="openpyxl")

files.download(out)

# ==================== RESULTADOS ====================
tiempo_total = time.time() - t0
print(f"\n{'='*80}")
print(f"‚úÖ ¬°AN√ÅLISIS COMPLETADO!")
print(f"‚è±Ô∏è  Tiempo: {tiempo_total:.1f}s")
print(f"üìÑ Archivo: {out}")
print(f"{'='*80}\n")

# Estad√≠sticas
total = len(df)
pos = (df["tono_marca"] == "Positivo").sum()
neg = (df["tono_marca"] == "Negativo").sum()
neu = (df["tono_marca"] == "Neutro").sum()

print("üìä RESUMEN:\n")
print(f"   üìà Total noticias: {total}")
print(f"   üî¢ Grupos √∫nicos: {len(rep_texts)}")
print(f"   üîó Promedio noticias/grupo: {total/len(rep_texts):.1f}")
print(f"   ‚úÖ Positivas: {pos} ({pos/total*100:.1f}%)")
print(f"   ‚ùå Negativas: {neg} ({neg/total*100:.1f}%)")
print(f"   ‚ö™ Neutras: {neu} ({neu/total*100:.1f}%)")

# Mostrar algunos ejemplos de agrupaci√≥n
print("\nüìã EJEMPLOS DE AGRUPACI√ìN:")
grupos_multi = df[df.duplicated(subset=["grupo_id"], keep=False)].sort_values("grupo_id")
if len(grupos_multi) > 0:
    for gid in grupos_multi["grupo_id"].unique()[:2]:
        grupo = df[df["grupo_id"] == gid]
        print(f"\n   üîó Grupo {gid} ‚Üí {len(grupo)} noticias similares")
        print(f"      Tono asignado: {grupo.iloc[0]['tono_marca']}")
        print(f"      Tema asignado: {grupo.iloc[0]['tema']}")
        for idx, row in grupo.head(2).iterrows():
            texto_corto = row[text_col][:70] + "..." if len(row[text_col]) > 70 else row[text_col]
            print(f"      ‚Ä¢ {texto_corto}")
else:
    print("   ‚ÑπÔ∏è  No se encontraron noticias duplicadas en esta muestra")

print("\nüìà DISTRIBUCI√ìN DE TONO:")
print(df["tono_marca"].value_counts().to_string())

print("\nüéØ DISTRIBUCI√ìN DE TEMAS:")
print(df["tema"].value_counts().to_string())

print("\n‚ú® An√°lisis finalizado exitosamente ‚ú®")

"""#Analizador v2.1 H√çBRIDO
##Modelo de sentimiento: RoBERTa-spanish

##Modelo: finiteautomata/beto-sentiment-analysis
##Base: BETO (BERT espa√±ol)
"""

# @title üîß Instalaciones
!pip install -q torch transformers sentence-transformers pandas openpyxl tqdm scikit-learn 2>/dev/null

# @title Analizador v2.1 H√çBRIDO ‚Äì RoBERTa (tono) + E5 (clustering)
# @markdown ‚Ä¢ RoBERTa-spanish para an√°lisis de sentimiento preciso
# @markdown ‚Ä¢ E5 para clustering de noticias similares
# @markdown ‚Ä¢ M√°xima precisi√≥n en tono de marca

!pip install -q torch transformers sentence-transformers pandas openpyxl tqdm scikit-learn 2>/dev/null

import os, re, time, torch
import numpy as np
import pandas as pd
import warnings
from tqdm.auto import tqdm
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from collections import defaultdict
from google.colab import files

warnings.filterwarnings("ignore")

# ==================== CONFIGURACI√ìN ====================
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"üöÄ Dispositivo: {device.upper()}")

# MODELO 1: E5 para clustering (mantiene agrupaci√≥n r√°pida)
print("üì¶ Cargando multilingual-e5-large para clustering...")
embedder = SentenceTransformer("intfloat/multilingual-e5-large", device=device)

# MODELO 2: RoBERTa-spanish para an√°lisis de sentimiento
print("üì¶ Cargando RoBERTa-spanish para an√°lisis de tono...")
sentiment_model_name = "finiteautomata/beto-sentiment-analysis"
tokenizer = AutoTokenizer.from_pretrained(sentiment_model_name)
sentiment_model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_name)
sentiment_model = sentiment_model.to(device)
sentiment_model.eval()

print("‚úÖ Modelos cargados\n")

# ==================== REGLAS COMPLEMENTARIAS ====================
# Reglas que REFUERZAN el an√°lisis de RoBERTa

CLIENT_POSITIVE_PATTERNS = [
    "liderazgo del", "bajo el liderazgo", "gracias a la alcald√≠a",
    "gracias al alcalde", "gesti√≥n del alcalde", "alcald√≠a entrega",
    "alcalde confirma", "alcalde anuncia", "alcalde inaugura"
]

CLIENT_NEGATIVE_PATTERNS = [
    "investigan al alcalde", "denuncian al", "esc√°ndalo del alcalde",
    "corrupci√≥n del", "acusan al alcalde", "irregularidades del"
]

NEUTRAL_REACTIONS = [
    "lamenta", "rechaza atentado", "condena", "expresa solidaridad",
    "pide", "solicita", "manifiesta preocupaci√≥n"
]

ACHIEVEMENT_INDICATORS = [
    "bachilleres inician", "estudiantes se inscriben", "familias beneficiadas",
    "viviendas entregadas", "obras culminadas", "inversi√≥n de",
    "programas profesionales", "vida universitaria", "acceso a"
]

TOPICS_KEYWORDS = {
    "Infraestructura": ["escenario", "multideportivo", "obra", "infraestructura", "v√≠a", "parque",
                        "pavimentaci√≥n", "construcci√≥n", "puente", "polideportivo", "cancha"],
    "Educaci√≥n": ["educaci√≥n", "bachilleres", "universitaria", "colegio", "becas", "estudiantes",
                  "escuela", "universidad", "educaci√≥n superior", "acceso a la universidad"],
    "Seguridad y Justicia": ["polic√≠a", "seguridad", "auxiliares", "dotaci√≥n", "homicidio",
                            "delincuencia", "crimen", "atentado", "fallecimiento"],
    "Relaciones Internacionales": ["embajador", "cumbre", "celac", "diplom√°tico", "internacional", "bilateral"],
    "Corrupci√≥n y Esc√°ndalos": ["esc√°ndalo", "corrupci√≥n", "investigan al", "denuncia contra", "fraude"],
    "Salud": ["salud", "hospital", "m√©dico", "atenci√≥n m√©dica", "vacunaci√≥n", "pandemia"],
    "Medio Ambiente": ["lluvias", "inundaciones", "emergencia", "clima", "ambiental", "desastre natural",
                      "hurac√°n", "afectados", "coletazo"],
    "Econom√≠a": ["econom√≠a", "empleo", "empresa", "comercio", "inversi√≥n econ√≥mica", "presupuesto"],
}

# ==================== FUNCIONES DE CLUSTERING (E5) ====================

def normalize_text(text):
    """Normaliza texto removiendo art√≠culos y preposiciones"""
    t = text.lower()
    stopwords = ['el', 'la', 'los', 'las', 'un', 'una', 'unos', 'unas',
                 'de', 'del', 'en', 'para', 'por', 'con', 'sin', 'sobre',
                 'que', 'todo', 'est√°', 'son', 'es']

    t = re.sub(r'[^\w√°√©√≠√≥√∫√±\s]', ' ', t)
    t = re.sub(r'\s+', ' ', t).strip()

    words = t.split()
    words = [w for w in words if w not in stopwords and len(w) > 2]

    return " ".join(words)

def get_signature(text, words=20):
    """Genera firma textual normalizada para clustering"""
    normalized = normalize_text(text)
    return " ".join(normalized.split()[:words])

def universal_cluster(texts):
    """Clustering usando E5 (mantiene agrupaci√≥n r√°pida y efectiva)"""
    print("üìä Agrupando noticias similares con E5...")
    groups = defaultdict(list)

    for i, t in enumerate(texts):
        signature = get_signature(t)
        groups[signature].append(i)

    representatives = []
    cluster_map = [-1] * len(texts)
    cid = 0

    for idxs in groups.values():
        longest_idx = max(idxs, key=lambda i: len(texts[i]))
        representatives.append(texts[longest_idx])

        for i in idxs:
            cluster_map[i] = cid
        cid += 1

    print(f"‚úÖ {len(texts)} noticias ‚Üí {len(representatives)} grupos √∫nicos")
    return representatives, np.array(cluster_map)

# ==================== AN√ÅLISIS CON ROBERTA ====================

def get_roberta_sentiment(text, max_length=512):
    """Obtiene sentimiento base usando RoBERTa"""
    # Truncar texto si es muy largo
    text_truncated = text[:max_length * 4]  # Aproximado para tokens

    inputs = tokenizer(text_truncated, return_tensors="pt",
                      truncation=True, max_length=max_length,
                      padding=True)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = sentiment_model(**inputs)
        logits = outputs.logits
        probs = torch.softmax(logits, dim=1)

    # BETO sentiment: 0=NEG, 1=NEU, 2=POS
    sentiment_idx = torch.argmax(probs, dim=1).item()
    confidence = probs[0][sentiment_idx].item()

    sentiment_map = {0: "Negativo", 1: "Neutro", 2: "Positivo"}

    return sentiment_map[sentiment_idx], confidence

def analyze_with_roberta(rep_text, client_name):
    """An√°lisis h√≠brido: RoBERTa + reglas contextuales"""
    lower = rep_text.lower()
    client_lower = client_name.lower()

    # PASO 1: Obtener sentimiento base de RoBERTa
    base_sentiment, base_confidence = get_roberta_sentiment(rep_text)

    # PASO 2: Ajustes contextuales espec√≠ficos para marca

    # Caso 1: Negativos DIRECTOS contra el cliente (override RoBERTa)
    if any(pattern in lower for pattern in CLIENT_NEGATIVE_PATTERNS):
        return "Negativo", 0.95, base_sentiment

    # Caso 2: Reacciones neutras (override solo si RoBERTa marc√≥ positivo/negativo)
    if any(reaction in lower for reaction in NEUTRAL_REACTIONS):
        # El cliente solo reacciona, no gestiona
        if base_sentiment != "Neutro":
            return "Neutro", 0.88, base_sentiment

    # Caso 3: POSITIVOS claros de gesti√≥n (reforzar si RoBERTa es neutro)
    client_is_positive_subject = any(pattern in lower for pattern in CLIENT_POSITIVE_PATTERNS)
    has_achievements = any(indicator in lower for indicator in ACHIEVEMENT_INDICATORS)

    if client_is_positive_subject or has_achievements:
        if base_sentiment == "Neutro":
            # RoBERTa no capt√≥ el positivo, pero el cliente S√ç est√° gestionando
            return "Positivo", 0.87, base_sentiment
        elif base_sentiment == "Positivo":
            # RoBERTa capt√≥ correctamente, aumentamos confianza
            return "Positivo", min(0.95, base_confidence + 0.10), base_sentiment

    # PASO 3: Usar an√°lisis de RoBERTa como base
    return base_sentiment, round(base_confidence, 3), base_sentiment

def get_topic(text):
    """An√°lisis de tema por keywords"""
    lower = text.lower()
    tema = "Gesti√≥n y Acciones"
    max_matches = 0

    for topic, keywords in TOPICS_KEYWORDS.items():
        matches = sum(1 for kw in keywords if kw in lower)
        if matches > max_matches:
            max_matches = matches
            tema = topic

    return tema

# ==================== MAIN EXECUTION ====================
print("\n" + "="*80)
print("ü§ñ ANALIZADOR H√çBRIDO v31 - RoBERTa + E5")
print("="*80 + "\n")

client_name = input("üë§ Nombre del cliente (ej: Carlos Pinedo): ").strip()
if not client_name:
    client_name = "Carlos Pinedo"
    print(f"   Usando cliente por defecto: {client_name}")

print("\nüìÇ Sube tu archivo Excel...")
uploaded = files.upload()
file_name = list(uploaded.keys())[0]

print(f"üìñ Leyendo {file_name}...")
df = pd.read_excel(file_name)

text_columns = ['resumen', 'texto', 'contenido', 'titulo', 'title', 'noticia', 'descripcion']
text_col = next((c for c in text_columns if c in df.columns), None)

if not text_col:
    print(f"‚ùå Error: No se encontr√≥ columna de texto. Columnas disponibles: {list(df.columns)}")
    raise ValueError("No se encontr√≥ columna de texto v√°lida")

print(f"‚úÖ Columna detectada: '{text_col}'")

df = df.dropna(subset=[text_col]).reset_index(drop=True)
texts = df[text_col].astype(str).tolist()
print(f"üìä {len(texts)} noticias cargadas\n")

# ==================== PROCESAMIENTO ====================
t0 = time.time()

# PASO 1: Clustering con E5
rep_texts, cluster_map = universal_cluster(texts)

# PASO 2: An√°lisis con RoBERTa + reglas
print("\nü§ñ Analizando tono con RoBERTa-spanish...")
tonos = []
confianzas = []
tonos_roberta_base = []  # Para comparaci√≥n
temas = []

for rep in tqdm(rep_texts, desc="Procesando"):
    tono, conf, base_sent = analyze_with_roberta(rep, client_name)
    tema = get_topic(rep)

    tonos.append(tono)
    confianzas.append(conf)
    tonos_roberta_base.append(base_sent)
    temas.append(tema)

# PASO 3: Mapear a todas las noticias del grupo
print("\nüîó Aplicando resultados a grupos similares...")
df["grupo_id"] = cluster_map
df["tono_marca"] = [tonos[cluster_map[i]] for i in range(len(df))]
df["confianza"] = [confianzas[cluster_map[i]] for i in range(len(df))]
df["tema"] = [temas[cluster_map[i]] for i in range(len(df))]
df["tono_roberta_base"] = [tonos_roberta_base[cluster_map[i]] for i in range(len(df))]

# ==================== EXPORTACI√ìN ====================
out = f"Final_RoBERTa_{client_name.replace(' ', '_')}_{time.strftime('%Y%m%d_%H%M')}.xlsx"
print(f"\nüíæ Guardando resultados en {out}...")

df.to_excel(out, sheet_name="Resultados", index=False, engine="openpyxl")
files.download(out)

# ==================== RESULTADOS ====================
tiempo_total = time.time() - t0
print(f"\n{'='*80}")
print(f"‚úÖ ¬°AN√ÅLISIS COMPLETADO CON RoBERTa!")
print(f"‚è±Ô∏è  Tiempo: {tiempo_total:.1f}s")
print(f"üìÑ Archivo: {out}")
print(f"{'='*80}\n")

total = len(df)
pos = (df["tono_marca"] == "Positivo").sum()
neg = (df["tono_marca"] == "Negativo").sum()
neu = (df["tono_marca"] == "Neutro").sum()

# Comparaci√≥n: ajustes aplicados
ajustes = (df["tono_marca"] != df["tono_roberta_base"]).sum()

print("üìä RESUMEN:\n")
print(f"   üìà Total noticias: {total}")
print(f"   üî¢ Grupos √∫nicos: {len(rep_texts)}")
print(f"   üîó Promedio noticias/grupo: {total/len(rep_texts):.1f}")
print(f"   ‚úÖ Positivas: {pos} ({pos/total*100:.1f}%)")
print(f"   ‚ùå Negativas: {neg} ({neg/total*100:.1f}%)")
print(f"   ‚ö™ Neutras: {neu} ({neu/total*100:.1f}%)")
print(f"\n   üîß Ajustes contextuales aplicados: {ajustes} ({ajustes/total*100:.1f}%)")

print("\nüìà DISTRIBUCI√ìN DE TONO FINAL:")
print(df["tono_marca"].value_counts().to_string())

print("\nü§ñ TONO BASE ROBERTA (sin ajustes):")
print(df["tono_roberta_base"].value_counts().to_string())

print("\nüéØ DISTRIBUCI√ìN DE TEMAS:")
print(df["tema"].value_counts().to_string())

# Mostrar ejemplos de ajustes
if ajustes > 0:
    print("\nüîç EJEMPLOS DE AJUSTES CONTEXTUALES:")
    adjusted = df[df["tono_marca"] != df["tono_roberta_base"]].head(3)
    for idx, row in adjusted.iterrows():
        texto_corto = row[text_col][:90] + "..." if len(row[text_col]) > 90 else row[text_col]
        print(f"\n   üì∞ {texto_corto}")
        print(f"      RoBERTa base: {row['tono_roberta_base']} ‚Üí Final: {row['tono_marca']}")

print("\n‚ú® An√°lisis finalizado exitosamente ‚ú®")